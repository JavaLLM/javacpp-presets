// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.javallm.llamacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.javallm.llamacpp.global.llama.*;


    // optimization parameters
    //
    //   see ggml.c (ggml_opt_default_params) for default values
    //
    @Properties(inherit = org.javallm.llamacpp.presets.llama.class)
public class ggml_opt_params extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public ggml_opt_params() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public ggml_opt_params(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public ggml_opt_params(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public ggml_opt_params position(long position) {
            return (ggml_opt_params)super.position(position);
        }
        @Override public ggml_opt_params getPointer(long i) {
            return new ggml_opt_params((Pointer)this).offsetAddress(i);
        }
    
        public native @Cast("ggml_opt_type") int type(); public native ggml_opt_params type(int setter);

        public native int n_threads(); public native ggml_opt_params n_threads(int setter);

        // delta-based convergence test
        //
        //   if past == 0 - disabled
        //   if past > 0:
        //     stop if |f(x) - f(x_past)| < delta * max(1, |f(x)|)
        //
        public native int past(); public native ggml_opt_params past(int setter);
        public native float delta(); public native ggml_opt_params delta(float setter);

        // maximum number of iterations without improvement
        //
        //   if 0 - disabled
        //   if > 0:
        //     assume convergence if no cost improvement in this number of iterations
        //
        public native int max_no_improvement(); public native ggml_opt_params max_no_improvement(int setter);

        public native @Cast("bool") boolean print_forward_graph(); public native ggml_opt_params print_forward_graph(boolean setter);
        public native @Cast("bool") boolean print_backward_graph(); public native ggml_opt_params print_backward_graph(boolean setter);

        // ADAM parameters
            @Name("adam.n_iter") public native int adam_n_iter(); public native ggml_opt_params adam_n_iter(int setter);

            @Name("adam.sched") public native float adam_sched(); public native ggml_opt_params adam_sched(float setter); // schedule multiplier (fixed, decay or warmup)
            @Name("adam.decay") public native float adam_decay(); public native ggml_opt_params adam_decay(float setter); // weight decay for AdamW, use 0.0f to disable
            @Name("adam.alpha") public native float adam_alpha(); public native ggml_opt_params adam_alpha(float setter); // learning rate
            @Name("adam.beta1") public native float adam_beta1(); public native ggml_opt_params adam_beta1(float setter);
            @Name("adam.beta2") public native float adam_beta2(); public native ggml_opt_params adam_beta2(float setter);
            @Name("adam.eps") public native float adam_eps(); public native ggml_opt_params adam_eps(float setter);   // epsilon for numerical stability
            @Name("adam.eps_f") public native float adam_eps_f(); public native ggml_opt_params adam_eps_f(float setter); // epsilon for convergence test
            @Name("adam.eps_g") public native float adam_eps_g(); public native ggml_opt_params adam_eps_g(float setter); // epsilon for convergence test

        // LBFGS parameters
            @Name("lbfgs.m") public native int lbfgs_m(); public native ggml_opt_params lbfgs_m(int setter); // number of corrections to approximate the inv. Hessian
            @Name("lbfgs.n_iter") public native int lbfgs_n_iter(); public native ggml_opt_params lbfgs_n_iter(int setter);
            @Name("lbfgs.max_linesearch") public native int lbfgs_max_linesearch(); public native ggml_opt_params lbfgs_max_linesearch(int setter);

            @Name("lbfgs.eps") public native float lbfgs_eps(); public native ggml_opt_params lbfgs_eps(float setter);      // convergence tolerance
            @Name("lbfgs.ftol") public native float lbfgs_ftol(); public native ggml_opt_params lbfgs_ftol(float setter);     // line search tolerance
            @Name("lbfgs.wolfe") public native float lbfgs_wolfe(); public native ggml_opt_params lbfgs_wolfe(float setter);
            @Name("lbfgs.min_step") public native float lbfgs_min_step(); public native ggml_opt_params lbfgs_min_step(float setter);
            @Name("lbfgs.max_step") public native float lbfgs_max_step(); public native ggml_opt_params lbfgs_max_step(float setter);

            @Name("lbfgs.linesearch") public native @Cast("ggml_linesearch") int lbfgs_linesearch(); public native ggml_opt_params lbfgs_linesearch(int setter);
    }
